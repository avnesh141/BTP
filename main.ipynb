{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1fdbee-a1c8-4ec5-a395-baae0dd3ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596942f0-603c-4214-a5a3-2f8c7f02aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_class=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e123f9-e894-4bb6-a467-f80ac4c8e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Train_Input')\n",
    "TRAIN_TARGET_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Train_Output')\n",
    "TEST_NOISY_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Test_Input')\n",
    "TEST_CLEAN_DIR = Path('Datasets/clean_testset_wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4ee461-5cd6-4019-8910-c6b7e8b7dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000 \n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261a7cb8-e105-4f91-9c83-950b6ebe0498",
   "metadata": {},
   "outputs": [],
   "source": [
    " class SpeechDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, noisy_files, target_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.target_files = sorted(target_files)\n",
    "        \n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        self.max_len = 150000\n",
    "     \n",
    "    \n",
    "    def _len(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        print(_)\n",
    "        return waveform\n",
    "  \n",
    "    def getitem(self, index):\n",
    "\n",
    "        file_t=self.target_files[index]\n",
    "        file_n=self.noisy_files[index]\n",
    "        file_t=str(file_t)\n",
    "        file_n=str(file_n)\n",
    "        x_target = self.load_sample(file_t)\n",
    "        x_noisy = self.load_sample(file_n)\n",
    "        \n",
    "        x_target = self.prepare_sample(x_target)\n",
    "        x_noisy = self.prepare_sample(x_noisy)\n",
    "        \n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True,return_complex=True)\n",
    "        x_target_stft = torch.stft(input=x_target, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True,return_complex=True)\n",
    "        # return 0\n",
    "        return x_noisy_stft, x_target_stft\n",
    "        \n",
    "    def prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        # print(waveform.shape)\n",
    "        current_len = waveform.shape[1]\n",
    "        # print(current_len)\n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        # print(output.shape)\n",
    "        output[0, -current_len:] = waveform[0,:self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176918c9-d2db-4db1-94b7-f5bd987bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob('*.wav')))\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob('*.wav')))\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))\n",
    "\n",
    "# print(\"No. of Training files:\",len(train_input_files))\n",
    "# print(\"No. of Testing files:\",len(test_noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504be1ac-b4da-46d9-8aac-109d010ff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_input_files, train_target_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b927dd1-77b3-48db-8acf-83c50cc7a4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5735650-6123-4d22-857d-c21406847858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516694e8-9dd0-4408-b6da-c7e964113f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
