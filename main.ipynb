{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1654,
   "id": "df1fdbee-a1c8-4ec5-a395-baae0dd3ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "%run metrics.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import  pyplot as plt\n",
    "from pypesq import pesq\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# not everything is smooth in sklearn, to conveniently output images in colab\n",
    "# we will ignore warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "id": "596942f0-603c-4214-a5a3-2f8c7f02aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_class=3\n",
    "training_type =  \"Noise2Noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "id": "0f7531fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "basepath = str(noise_class)+\"_\"+training_type\n",
    "os.makedirs(basepath,exist_ok=True)\n",
    "os.makedirs(basepath+\"/Weights\",exist_ok=True)\n",
    "os.makedirs(basepath+\"/Samples\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "id": "e4e123f9-e894-4bb6-a467-f80ac4c8e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Train_Input')\n",
    "TRAIN_TARGET_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Train_Output')\n",
    "TEST_NOISY_DIR = Path('Datasets/US_Class'+str(noise_class)+'_Test_Input')\n",
    "TEST_CLEAN_DIR = Path('Datasets/clean_testset_wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "id": "70d0894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "id": "6f4ee461-5cd6-4019-8910-c6b7e8b7dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000 \n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "id": "261a7cb8-e105-4f91-9c83-950b6ebe0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, noisy_files, target_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.target_files = sorted(target_files)\n",
    "        \n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        self.max_len = 165000\n",
    "     \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        # print(_)\n",
    "        return waveform\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_t=self.target_files[index]\n",
    "        file_n=self.noisy_files[index]\n",
    "        file_t=str(file_t)\n",
    "        file_n=str(file_n)\n",
    "        x_target = self.load_sample(file_t)\n",
    "        x_noisy = self.load_sample(file_n)\n",
    "        \n",
    "        x_target = self._prepare_sample(x_target)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "        \n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, hop_length=self.hop_length,window=None, normalized=True,return_complex=True)\n",
    "        x_target_stft = torch.stft(input=x_target, n_fft=self.n_fft, hop_length=self.hop_length,window=None, normalized=True,return_complex=True)\n",
    "        # return 0\n",
    "        # print(x_noisy_stft)\n",
    "        return torch.view_as_real(x_noisy_stft), torch.view_as_real(x_target_stft)\n",
    "        \n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        # print(waveform.shape)\n",
    "        current_len = waveform.shape[1]\n",
    "        # print(current_len)\n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        # print(output.shape)\n",
    "        output[0, -current_len:] = waveform[0,:self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "id": "176918c9-d2db-4db1-94b7-f5bd987bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob('*.wav')))\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob('*.wav')))\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))\n",
    "\n",
    "# print(\"No. of Training files:\",len(train_input_files))\n",
    "# print(\"No. of Testing files:\",len(test_noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "id": "504be1ac-b4da-46d9-8aac-109d010ff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_input_files, train_target_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "id": "04efc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "id": "16bd470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    win_length=16,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "id": "63ac5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "id": "3e085499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec=spectrogram(train_dataset.load_sample(\"Datasets/US_Class3_Test_Input/p232_001.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "id": "ba6f6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_spectrogram(spec[1], title=\"torchaudio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "id": "faadbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in test_loader:\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "id": "9b927dd1-77b3-48db-8acf-83c50cc7a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader=DataLoader(test_dataset,batch_size=1,shuffle=True)\n",
    "train_loader=DataLoader(train_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "id": "1f5410d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "        self.kernel_size=kernel_size\n",
    "        self.padding=padding\n",
    "        self.stride=stride\n",
    "        self.real_conv= nn.Conv2d(in_channels=self.in_channels,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  kernel_size=self.kernel_size,\n",
    "                                  padding=self.padding,\n",
    "                                  stride=self.stride)\n",
    "        self.imag_conv = nn.Conv2d(in_channels=self.in_channels,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  kernel_size=self.kernel_size,\n",
    "                                  padding=self.padding,\n",
    "                                  stride=self.stride)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.imag_conv.weight)\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        x_real = x[...,0]\n",
    "        x_imag= x[...,1]\n",
    "        c_real = self.real_conv(x_real)-self.imag_conv(x_imag)\n",
    "        c_imag=self.imag_conv(x_real)+self.real_conv(x_imag)\n",
    "\n",
    "        output = torch.stack([c_real,c_imag],dim=-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "id": "516694e8-9dd0-4408-b6da-c7e964113f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride,out_padding=0,padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "        self.kernel_size=kernel_size\n",
    "        self.padding=padding\n",
    "        self.out_padding=out_padding\n",
    "        self.stride=stride\n",
    "        \n",
    "        self.real_convt= nn.ConvTranspose2d(in_channels=self.in_channels,\n",
    "                                            out_channels=self.out_channels,\n",
    "                                            kernel_size=self.kernel_size,\n",
    "                                            stride=self.stride,\n",
    "                                            output_padding=self.out_padding,\n",
    "                                            padding=self.padding)\n",
    "        self.imag_convt=nn.ConvTranspose2d(in_channels=self.in_channels,\n",
    "                                            out_channels=self.out_channels,\n",
    "                                            kernel_size=self.kernel_size,\n",
    "                                            stride=self.stride,\n",
    "                                            output_padding=self.out_padding,\n",
    "                                            padding=self.padding)\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.imag_convt.weight)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        x_real=x[...,0]\n",
    "        x_imag=x[...,1]\n",
    "\n",
    "        c_real=self.real_convt(x_real)-self.imag_convt(x_imag)\n",
    "        c_imag=self.imag_convt(x_real)+self.real_convt(x_imag)\n",
    "            \n",
    "        output=torch.stack([c_real,c_imag],dim=-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "id": "2bc31d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBatchNorm2D(nn.Module):\n",
    "    def __init__(self,num_features,eps=1e-05,momentum=0.1,affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        self.num_features=num_features\n",
    "        self.affine=affine\n",
    "        self.momentum=momentum\n",
    "        self.eps=eps\n",
    "        self.track_running_stats=track_running_stats\n",
    "\n",
    "        self.real_b=nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.imag_b=nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        real=x[...,0]\n",
    "        imag=x[...,1]\n",
    "\n",
    "        real_cb=self.real_b(real)\n",
    "        imag_cb=self.imag_b(imag)\n",
    "\n",
    "        output=torch.stack([real_cb,imag_cb],dim=-1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "id": "4aed2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,in_channels=1,out_channels=45,stride=(2,2),kernel_size=(7,5),padding=(0,0)):\n",
    "        super().__init__()\n",
    "        self.kernel_size=kernel_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "\n",
    "        self.cconv= CConv2d(in_channels=self.in_channels,\n",
    "                            out_channels=self.out_channels,\n",
    "                            kernel_size=self.kernel_size,\n",
    "                            stride=self.stride,\n",
    "                            padding=self.padding)\n",
    "        self.cbn=CBatchNorm2D(num_features=self.out_channels)\n",
    "\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "    def forward(self,x):\n",
    "        conved=self.cconv(x)\n",
    "        normed=self.cbn(conved)\n",
    "        acted=self.leaky_relu(normed)\n",
    "        # print(acted.shape)\n",
    "        return acted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "id": "e103f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride,padding,out_padding,last_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride_size = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.out_padding = out_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, \n",
    "                                       out_channels=self.out_channels, \n",
    "                             kernel_size=self.kernel_size,\n",
    "                               stride=self.stride_size, \n",
    "                               out_padding=self.out_padding, \n",
    "                               padding=self.padding\n",
    "                               )\n",
    "        \n",
    "        self.cbn = CBatchNorm2D(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1675,
   "id": "73f37211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.set_size(model_complexity=int(45//1.414), input_channels=1, model_depth=20)\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2\n",
    "        \n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
    "                             kernel_size=self.enc_kernel_sizes[i], stride=self.enc_strides[i], padding=self.enc_paddings[i])\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 kernel_size=self.dec_kernel_sizes[i], stride=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 out_padding=self.dec_output_padding[i],last_layer=False)\n",
    "            else:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 kernel_size=self.dec_kernel_sizes[i], stride=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 out_padding=self.dec_output_padding[i], last_layer=True)\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "       \n",
    "        \n",
    "    def forward(self, x, is_istft=True):\n",
    "        # print('x : ', x.shape)\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            # print('Encoder : ', x.shape)\n",
    "            \n",
    "        p = x\n",
    "        # print(x.shape)\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            # print('Decoder : ', p.shape)\n",
    "            p = torch.cat([p,xs[self.model_length - 1 - i]], dim=1)\n",
    "        \n",
    "        # u9 - the mask\n",
    "        \n",
    "        mask = p\n",
    "        \n",
    "        # print('mask : ', mask.shape)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        output = mask * orig_x\n",
    "        output = torch.squeeze(output, 1)\n",
    "        output=torch.view_as_complex(output)\n",
    "        # print(output.shape)\n",
    "        if is_istft:\n",
    "            output = torch.istft(output, n_fft=self.n_fft, hop_length=self.hop_length,window=None,return_complex=False, normalized=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [input_channels,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 128]\n",
    "\n",
    "            self.enc_kernel_sizes = [(7, 1),\n",
    "                                     (1, 7),\n",
    "                                     (6, 4),\n",
    "                                     (7, 5),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3)]\n",
    "\n",
    "            self.enc_strides = [(1, 1),\n",
    "                                (1, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1)]\n",
    "\n",
    "            self.enc_paddings = [(3, 0),\n",
    "                                 (0, 3),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0)]\n",
    "\n",
    "            self.dec_channels = [0,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 1]\n",
    "\n",
    "            self.dec_kernel_sizes = [(6, 3), \n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (8, 5),\n",
    "                                     (7, 5),\n",
    "                                     (1, 7),\n",
    "                                     (7, 1)]\n",
    "\n",
    "            self.dec_strides = [(2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (1, 1),\n",
    "                                (1, 1)]\n",
    "\n",
    "            self.dec_paddings = [(0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 3),\n",
    "                                 (3, 0)]\n",
    "            \n",
    "            self.dec_output_padding = [(0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0)]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1676,
   "id": "48a1ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pesq import pesq\n",
    "from scipy import interpolate\n",
    "\n",
    "def resample(original, old_rate, new_rate):\n",
    "    if old_rate != new_rate:\n",
    "        duration = original.shape[0] / old_rate\n",
    "        time_old  = np.linspace(0, duration, original.shape[0])\n",
    "        time_new  = np.linspace(0, duration, int(original.shape[0] * new_rate / old_rate))\n",
    "        interpolator = interpolate.interp1d(time_old, original.T)\n",
    "        new_audio = interpolator(time_new).T\n",
    "        return new_audio\n",
    "    else:\n",
    "        return original\n",
    "\n",
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "   \n",
    "    y_true_ = torch.squeeze(y_true_, 1)\n",
    "    y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True,window=None,return_complex=True)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    x = torch.istft(x_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True,window=None,return_complex=True)\n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)\n",
    "wonky_samples = []\n",
    "def getMetricsonLoader(loader, net, use_net=True):\n",
    "    net.eval()\n",
    "    \n",
    "    scale_factor = 32768\n",
    "    metric_names = [\"PESQ-WB\",\"PESQ-NB\",\"SNR\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(5)]\n",
    "    for i, data in enumerate(loader):\n",
    "        if (i+1)%10==0:\n",
    "            end_str = \"\\n\"\n",
    "        else:\n",
    "            end_str = \",\"\n",
    "\n",
    "        if i in wonky_samples:\n",
    "            print(\"Something's up with this sample. Passing...\")\n",
    "        else:\n",
    "            noisy = data[0]\n",
    "            clean = data[1]\n",
    "            if use_net: \n",
    "                x_est = net(noisy.to(DEVICE), is_istft=True)\n",
    "                x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
    "            else:\n",
    "                x_est_np = torch.istft(torch.squeeze(noisy, 1), n_fft=N_FFT, hop_length=HOP_LENGTH,window=None,return_complex=True, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            x_clean_np = torch.istft(torch.squeeze(clean, 1), n_fft=N_FFT, hop_length=HOP_LENGTH,window=None,return_complex=True, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            \n",
    "        \n",
    "            metrics = AudioMetrics2(x_clean_np, x_est_np, 48000)\n",
    "            \n",
    "            ref_wb = resample(x_clean_np, 48000, 16000)\n",
    "            deg_wb = resample(x_est_np, 48000, 16000)\n",
    "            pesq_wb = pesq(16000, ref_wb, deg_wb, 'wb')\n",
    "            \n",
    "            ref_nb = resample(x_clean_np, 48000, 8000)\n",
    "            deg_nb = resample(x_est_np, 48000, 8000)\n",
    "            pesq_nb = pesq(8000, ref_nb, deg_nb, 'nb')\n",
    "\n",
    "            \n",
    "\n",
    "            overall_metrics[0].append(pesq_wb)\n",
    "            overall_metrics[1].append(pesq_nb)\n",
    "            overall_metrics[2].append(metrics.SNR)\n",
    "            overall_metrics[3].append(metrics.SSNR)\n",
    "            overall_metrics[4].append(metrics.STOI)\n",
    "    print()\n",
    "    print(\"Sample metrics computed\")\n",
    "    results = {}\n",
    "    for i in range(5):\n",
    "        temp = {}\n",
    "        temp[\"Mean\"] =  np.mean(overall_metrics[i])\n",
    "        temp[\"STD\"]  =  np.std(overall_metrics[i])\n",
    "        temp[\"Min\"]  =  min(overall_metrics[i])\n",
    "        temp[\"Max\"]  =  max(overall_metrics[i])\n",
    "        results[metric_names[i]] = temp\n",
    "    print(\"Averages computed\")\n",
    "    if use_net:\n",
    "        addon = \"(cleaned by model)\"\n",
    "    else:\n",
    "        addon = \"(pre denoising)\"\n",
    "    print(\"Metrics on test data\",addon)\n",
    "    for i in range(5):\n",
    "        print(\"{} : {:.3f}+/-{:.3f}\".format(metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1677,
   "id": "11f1f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.\n",
    "    counter = 0\n",
    "    try:\n",
    "        for noisy_x, clean_x in train_loader:\n",
    "\n",
    "            noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "        # print(noisy_x)\n",
    "            pred_x = net(noisy_x)\n",
    "            if pred_x==None:\n",
    "                continue\n",
    "            if noisy_x==None:\n",
    "                continue\n",
    "            if clean_x==None:\n",
    "                continue\n",
    "            loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_ep_loss += loss.item() \n",
    "            counter += 1\n",
    "\n",
    "        if counter!=0:\n",
    "            test_ep_loss /= counter\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return train_ep_loss\n",
    "    except:\n",
    "        train_ep_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1678,
   "id": "658b1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
    "    try:    \n",
    "        net.eval()\n",
    "        test_ep_loss = 0.\n",
    "        counter = 0.\n",
    "    \n",
    "        for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        # try:\n",
    "\n",
    "            noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "            pred_x = net(noisy_x)\n",
    "        \n",
    "            if pred_x==None:\n",
    "                continue\n",
    "            if noisy_x==None:\n",
    "                continue\n",
    "            if clean_x==None:\n",
    "                continue\n",
    "        # calculate loss\n",
    "            loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        # Calc the metrics here\n",
    "            test_ep_loss += loss.item() \n",
    "        \n",
    "            counter += 1\n",
    "     \n",
    "        testmet = getMetricsonLoader(test_loader,net,use_net)\n",
    "        if counter!=0:\n",
    "            test_ep_loss /= counter\n",
    "    \n",
    "    \n",
    "    #print(\"Actual compute done...testing now\")\n",
    "    \n",
    "\n",
    "    # clear cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        return test_ep_loss, testmet\n",
    "    except:\n",
    "        test_ep_loss,{}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1679,
   "id": "2ecf04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    try:\n",
    "        \n",
    "    # print(\"eme\")\n",
    "        for e in tqdm(range(epochs)):\n",
    "        \n",
    "        # print(\"sms\")\n",
    "            train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "            test_loss = 0\n",
    "            scheduler.step()\n",
    "            print(\"Saving model....\")\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                test_loss, testmet = test_epoch(net, test_loader, loss_fn,use_net=True)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        #print(\"skipping testing cuz peak autism idk\")\n",
    "        \n",
    "            with open(basepath + \"/results.txt\",\"a\") as f:\n",
    "                f.write(\"Epoch :\"+str(e+1) + \"\\n\" + str(testmet))\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "            print(\"OPed to txt\")\n",
    "        \n",
    "            torch.save(net.state_dict(), basepath +'/Weights/dc20_model_'+str(e+1)+'.pth')\n",
    "            torch.save(optimizer.state_dict(), basepath+'/Weights/dc20_opt_'+str(e+1)+'.pth')\n",
    "        \n",
    "            print(\"Models saved\")\n",
    "\n",
    "        # clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        #print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        #              \"Loss: {:.6f}...\".format(train_loss),\n",
    "        #              \"Test Loss: {:.6f}\".format(test_loss))\n",
    "        return train_losses, test_losses\n",
    "    except:\n",
    "        return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1680,
   "id": "4b9ac492",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dcunet20 = DCUnet(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "loss_fn = wsdr_fn\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1681,
   "id": "c3ed1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses, test_losses = train(dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97178e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1682,
   "id": "9dc31d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_noisy_files = sorted(list(Path(\"Datasets/US_Class3_Test_Input\").rglob('*.wav')))\n",
    "# test_clean_files = sorted(list(Path(\"Samples/Sample_Test_Target\").rglob('*.wav')))\n",
    "\n",
    "# test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "\n",
    "# For testing purpose\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "# test_noisy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1683,
   "id": "1d975ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcunet20.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1684,
   "id": "bd518f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1693,
   "id": "3f9355e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Thanks\n",
      "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    }
   ],
   "source": [
    "# dcunet20.eval()\n",
    "for x_n,x_c in test_loader:\n",
    "    try:  \n",
    "        x_est=dcunet20(x_n,is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        x_c_np = torch.istft(torch.view_as_complex(torch.squeeze(x_c[0], 1)), n_fft=N_FFT, hop_length=HOP_LENGTH,return_complex=False,window=None, normalized=True).view(-1).detach().cpu().numpy()\n",
    "        x_n_np = torch.istft(torch.view_as_complex(torch.squeeze(x_n[0], 1)), n_fft=N_FFT, hop_length=HOP_LENGTH,return_complex=False,window=None, normalized=True).view(-1).detach().cpu().numpy()\n",
    "        metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "        # print(\"metrics.display()\")\n",
    "    except Exception as error:\n",
    "        # pass\n",
    "        print(error)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4469e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1686,
   "id": "eecc9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_n_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1687,
   "id": "62772ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_est_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1688,
   "id": "9da6413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_c_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060e6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
